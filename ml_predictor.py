"""
æœºå™¨å­¦ä¹ è‚¡ç¥¨é¢„æµ‹å™¨
ä½¿ç”¨ LSTM ç¥ç»ç½‘ç»œé¢„æµ‹è‚¡ç¥¨ä»·æ ¼è¶‹åŠ¿
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import warnings
warnings.filterwarnings('ignore')

class LSTMStockPredictor:
    """
    LSTM è‚¡ç¥¨é¢„æµ‹å™¨
    é¢„æµ‹è‚¡ç¥¨æœªæ¥æ¶¨è·Œæ¦‚ç‡
    """
    
    def __init__(self, sequence_length=30, epochs=50, batch_size=32):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨
        
        Args:
            sequence_length: è¾“å…¥åºåˆ—é•¿åº¦ï¼ˆå¤©æ•°ï¼‰
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
        """
        self.sequence_length = sequence_length
        self.epochs = epochs
        self.batch_size = batch_size
        self.model = None
        self.scaler = MinMaxScaler()
        self.feature_columns = ['open', 'high', 'low', 'close', 'volume', 'amount']
        
    def _prepare_features(self, df):
        """
        å‡†å¤‡ç‰¹å¾æ•°æ®
        
        Args:
            df: è‚¡ç¥¨æ•°æ®DataFrame
            
        Returns:
            å¤„ç†åçš„ç‰¹å¾æ•°æ®
        """
        data = df.copy()
        
        # å¤„ç†ç¼ºå¤±çš„ amount åˆ—
        if 'amount' not in data.columns or data['amount'].isna().all():
            # å¦‚æœæ²¡æœ‰æˆäº¤é¢æ•°æ®ï¼Œç”¨æˆäº¤é‡ * å¹³å‡ä»·æ ¼ä¼°ç®—
            data['amount'] = data['volume'] * (data['high'] + data['low']) / 2
        
        # å¡«å……ç¼ºå¤±å€¼
        data['amount'] = data['amount'].fillna(data['volume'] * data['close'])
        
        # åŸºç¡€ç‰¹å¾
        data['return_1d'] = data['close'].pct_change()
        data['return_5d'] = data['close'].pct_change(5)
        data['return_10d'] = data['close'].pct_change(10)
        
        # æŠ€æœ¯æŒ‡æ ‡ - ç§»åŠ¨å¹³å‡çº¿
        data['ma_5'] = data['close'].rolling(window=5).mean()
        data['ma_10'] = data['close'].rolling(window=10).mean()
        data['ma_20'] = data['close'].rolling(window=20).mean()
        data['ma_60'] = data['close'].rolling(window=60).mean()
        data['ma_ratio_5_20'] = data['ma_5'] / data['ma_20']
        data['ma_ratio_10_20'] = data['ma_10'] / data['ma_20']
        data['ma_ratio_20_60'] = data['ma_20'] / data['ma_60']
        
        # ç›¸å¯¹å¼ºå¼±æŒ‡æ•° (RSI) - å¤šå‘¨æœŸ
        def calculate_rsi(prices, period=14):
            delta = prices.diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs = gain / loss
            return 100 - (100 / (1 + rs))
        
        data['rsi_6'] = calculate_rsi(data['close'], 6)
        data['rsi_14'] = calculate_rsi(data['close'], 14)
        data['rsi_21'] = calculate_rsi(data['close'], 21)
        
        # å¸ƒæ—å¸¦
        bb_window = 20
        data['bb_middle'] = data['close'].rolling(window=bb_window).mean()
        bb_std = data['close'].rolling(window=bb_window).std()
        data['bb_upper'] = data['bb_middle'] + (bb_std * 2)
        data['bb_lower'] = data['bb_middle'] - (bb_std * 2)
        data['bb_position'] = (data['close'] - data['bb_lower']) / (data['bb_upper'] - data['bb_lower'])
        data['bb_width'] = (data['bb_upper'] - data['bb_lower']) / data['bb_middle']
        
        # MACD
        ema_12 = data['close'].ewm(span=12).mean()
        ema_26 = data['close'].ewm(span=26).mean()
        data['macd_dif'] = ema_12 - ema_26
        data['macd_dea'] = data['macd_dif'].ewm(span=9).mean()
        data['macd_histogram'] = data['macd_dif'] - data['macd_dea']
        data['macd_signal'] = (data['macd_dif'] > data['macd_dea']).astype(int)
        
        # KDJæŒ‡æ ‡
        low_min = data['low'].rolling(window=9).min()
        high_max = data['high'].rolling(window=9).max()
        rsv = (data['close'] - low_min) / (high_max - low_min) * 100
        data['kdj_k'] = rsv.ewm(alpha=1/3).mean()
        data['kdj_d'] = data['kdj_k'].ewm(alpha=1/3).mean()
        data['kdj_j'] = 3 * data['kdj_k'] - 2 * data['kdj_d']
        
        # æˆäº¤é‡æŒ‡æ ‡
        data['volume_ma_5'] = data['volume'].rolling(window=5).mean()
        data['volume_ma_20'] = data['volume'].rolling(window=20).mean()
        data['volume_ratio_5'] = data['volume'] / data['volume_ma_5']
        data['volume_ratio_20'] = data['volume'] / data['volume_ma_20']
        
        # ä»·æ ¼ä½ç½®å’ŒåŠ¨é‡
        data['price_position_20'] = (data['close'] - data['low'].rolling(window=20).min()) / \
                                   (data['high'].rolling(window=20).max() - data['low'].rolling(window=20).min())
        data['price_position_60'] = (data['close'] - data['low'].rolling(window=60).min()) / \
                                   (data['high'].rolling(window=60).max() - data['low'].rolling(window=60).min())
        
        # æ³¢åŠ¨ç‡
        data['volatility_5'] = data['return_1d'].rolling(window=5).std()
        data['volatility_20'] = data['return_1d'].rolling(window=20).std()
        
        # é«˜ä½ä»·æ¯”ä¾‹
        data['high_low_ratio'] = data['high'] / data['low']
        data['close_open_ratio'] = data['close'] / data['open']
        
        # æˆäº¤é¢ç‰¹å¾
        data['amount_ma_20'] = data['amount'].rolling(window=20).mean()
        data['amount_ratio'] = data['amount'] / data['amount_ma_20']
        
        # é€‰æ‹©ç‰¹å¾åˆ— (æ‰©å±•åˆ°35ä¸ªç‰¹å¾)
        feature_cols = ['open', 'high', 'low', 'close', 'volume', 'amount',
                       'return_1d', 'return_5d', 'return_10d',
                       'ma_5', 'ma_10', 'ma_20', 'ma_60', 'ma_ratio_5_20', 'ma_ratio_10_20', 'ma_ratio_20_60',
                       'rsi_6', 'rsi_14', 'rsi_21', 'bb_position', 'bb_width',
                       'macd_dif', 'macd_dea', 'macd_histogram', 'macd_signal',
                       'kdj_k', 'kdj_d', 'kdj_j',
                       'volume_ratio_5', 'volume_ratio_20',
                       'price_position_20', 'price_position_60',
                       'volatility_5', 'volatility_20',
                       'high_low_ratio', 'close_open_ratio', 'amount_ratio']
        
        # åˆ é™¤ç¼ºå¤±å€¼å’Œæ— ç©·å€¼
        data = data[feature_cols].replace([np.inf, -np.inf], np.nan).dropna()
        
        return data
    
    def _create_sequences(self, data, target):
        """
        åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®
        
        Args:
            data: ç‰¹å¾æ•°æ® (numpy array)
            target: ç›®æ ‡å˜é‡ (pandas Series)
            
        Returns:
            X, y: åºåˆ—æ•°æ®å’Œæ ‡ç­¾
        """
        X, y = [], []
        
        # é‡ç½®targetçš„ç´¢å¼•ï¼Œä½¿å…¶ä»0å¼€å§‹
        target_values = target.values
        
        for i in range(self.sequence_length, len(data)):
            X.append(data[i-self.sequence_length:i])
            y.append(target_values[i])
            
        return np.array(X), np.array(y)
    
    def _build_model(self, input_shape):
        """
        æ„å»ºå¢å¼ºç‰ˆ LSTM æ¨¡å‹
        
        Args:
            input_shape: è¾“å…¥å½¢çŠ¶
            
        Returns:
            ç¼–è¯‘åçš„æ¨¡å‹
        """
        model = keras.Sequential([
            # ç¬¬ä¸€å±‚LSTMï¼Œæ›´å¤šç¥ç»å…ƒ
            layers.LSTM(128, return_sequences=True, input_shape=input_shape),
            layers.BatchNormalization(),
            layers.Dropout(0.3),
            
            # ç¬¬äºŒå±‚LSTM
            layers.LSTM(64, return_sequences=True),
            layers.BatchNormalization(),
            layers.Dropout(0.3),
            
            # ç¬¬ä¸‰å±‚LSTM
            layers.LSTM(32),
            layers.BatchNormalization(),
            layers.Dropout(0.3),
            
            # å…¨è¿æ¥å±‚
            layers.Dense(64, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.5),
            
            layers.Dense(32, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.5),
            
            layers.Dense(16, activation='relu'),
            layers.Dropout(0.3),
            
            layers.Dense(1, activation='sigmoid')  # äºŒåˆ†ç±»ï¼šæ¶¨/è·Œ
        ])
        
        # ä½¿ç”¨æ›´å¥½çš„ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦
        optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)
        
        model.compile(
            optimizer=optimizer,
            loss='binary_crossentropy',
            metrics=['accuracy', 'precision', 'recall']
        )
        
        return model
    
    def train(self, stock_data_dict, min_samples=1000):
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            stock_data_dict: è‚¡ç¥¨æ•°æ®å­—å…¸ {code: DataFrame}
            min_samples: æœ€å°æ ·æœ¬æ•°é‡
        """
        print("ğŸš€ å¼€å§‹è®­ç»ƒ LSTM è‚¡ç¥¨é¢„æµ‹æ¨¡å‹...")
        
        all_X, all_y = [], []
        processed_count = 0
        
        # å¤„ç†æ¯åªè‚¡ç¥¨çš„æ•°æ®
        for code, df in stock_data_dict.items():
            if len(df) < self.sequence_length + 50:  # ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®
                print(f"è·³è¿‡ {code}: æ•°æ®ä¸è¶³ ({len(df)} è¡Œ)")
                continue
                
            try:
                # å‡†å¤‡ç‰¹å¾
                features = self._prepare_features(df)
                if len(features) < self.sequence_length + 20:
                    print(f"è·³è¿‡ {code}: å¤„ç†åæ•°æ®ä¸è¶³ ({len(features)} è¡Œ)")
                    continue
                
                # åˆ›å»ºç›®æ ‡å˜é‡ï¼ˆæœªæ¥1æ—¥æ¶¨è·Œï¼‰
                target = (features['close'].shift(-1) > features['close']).astype(int)
                target = target[:-1]  # å»æ‰æœ€åä¸€ä¸ªNaN
                features = features[:-1]  # å¯¹åº”å»æ‰æœ€åä¸€è¡Œ
                
                # æ ‡å‡†åŒ–ç‰¹å¾
                features_scaled = self.scaler.fit_transform(features)
                
                # åˆ›å»ºåºåˆ—
                X, y = self._create_sequences(features_scaled, target)
                
                if len(X) > 0:
                    all_X.append(X)
                    all_y.append(y)
                    processed_count += 1
                    print(f"âœ… å¤„ç†å®Œæˆ {code}: {len(X)} ä¸ªæ ·æœ¬")
                else:
                    print(f"è·³è¿‡ {code}: æ— æ³•åˆ›å»ºåºåˆ—")
                    
            except Exception as e:
                print(f"å¤„ç†è‚¡ç¥¨ {code} æ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        print(f"ğŸ“Š æˆåŠŸå¤„ç† {processed_count} åªè‚¡ç¥¨")
        
        if not all_X:
            raise ValueError("æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®æ¥è®­ç»ƒæ¨¡å‹")
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        X = np.concatenate(all_X, axis=0)
        y = np.concatenate(all_y, axis=0)
        
        print(f"ğŸ“Š è®­ç»ƒæ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
        print(f"ğŸ“Š æ­£æ ·æœ¬æ¯”ä¾‹: {y.mean():.2%}")
        
        if len(X) < min_samples:
            print(f"âš ï¸  æ ·æœ¬æ•°é‡ {len(X)} å°äºæœ€å°è¦æ±‚ {min_samples}ï¼Œä½†ç»§ç»­è®­ç»ƒ")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œç»§ç»­è®­ç»ƒ
        
        # åˆ†å‰²è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # æ„å»ºæ¨¡å‹
        self.model = self._build_model((X.shape[1], X.shape[2]))
        
        # è®­ç»ƒæ¨¡å‹
        print("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
        history = self.model.fit(
            X_train, y_train,
            epochs=self.epochs,
            batch_size=self.batch_size,
            validation_split=0.2,
            verbose=1,
            callbacks=[
                keras.callbacks.EarlyStopping(
                    monitor='val_loss',
                    patience=10,
                    restore_best_weights=True
                )
            ]
        )
        
        # è¯„ä¼°æ¨¡å‹
        y_pred = (self.model.predict(X_test) > 0.5).astype(int)
        accuracy = accuracy_score(y_test, y_pred)
        
        print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        print(f"ğŸ“ˆ æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.2%}")
        print(f"ğŸ“Š åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test, y_pred))
        
        return history
    
    def predict_probability(self, df):
        """
        é¢„æµ‹è‚¡ç¥¨ä¸Šæ¶¨æ¦‚ç‡
        
        Args:
            df: è‚¡ç¥¨æ•°æ®DataFrame
            
        Returns:
            ä¸Šæ¶¨æ¦‚ç‡ (0-1)
        """
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨ train() æ–¹æ³•")
        
        try:
            # å‡†å¤‡ç‰¹å¾
            features = self._prepare_features(df)
            if len(features) < self.sequence_length:
                return 0.5  # æ•°æ®ä¸è¶³ï¼Œè¿”å›ä¸­æ€§æ¦‚ç‡
            
            # æ ‡å‡†åŒ–
            features_scaled = self.scaler.transform(features)
            
            # å–æœ€åä¸€ä¸ªåºåˆ—
            X = features_scaled[-self.sequence_length:].reshape(1, self.sequence_length, -1)
            
            # é¢„æµ‹
            prob = self.model.predict(X, verbose=0)[0][0]
            
            return float(prob)
            
        except Exception as e:
            print(f"é¢„æµ‹æ—¶å‡ºé”™: {e}")
            return 0.5  # å‡ºé”™æ—¶è¿”å›ä¸­æ€§æ¦‚ç‡
    
    def save_model(self, filepath):
        """ä¿å­˜æ¨¡å‹"""
        if self.model is not None:
            self.model.save(filepath)
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_model(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        self.model = keras.models.load_model(filepath)
        print(f"âœ… æ¨¡å‹å·²ä» {filepath} åŠ è½½")


class MLEnhancedSelector:
    """
    æœºå™¨å­¦ä¹ å¢å¼ºé€‰è‚¡å™¨
    ç»“åˆä¼ ç»ŸæŠ€æœ¯æŒ‡æ ‡å’ŒMLé¢„æµ‹
    """
    
    def __init__(self, base_selector, ml_predictor, ml_threshold=0.6, ml_weight=0.3):
        """
        åˆå§‹åŒ–MLå¢å¼ºé€‰è‚¡å™¨
        
        Args:
            base_selector: åŸºç¡€é€‰è‚¡å™¨
            ml_predictor: MLé¢„æµ‹å™¨
            ml_threshold: MLé¢„æµ‹é˜ˆå€¼
            ml_weight: MLæƒé‡
        """
        self.base_selector = base_selector
        self.ml_predictor = ml_predictor
        self.ml_threshold = ml_threshold
        self.ml_weight = ml_weight
    
    def select(self, date, data):
        """
        é€‰è‚¡æ–¹æ³•
        
        Args:
            date: é€‰è‚¡æ—¥æœŸ
            data: è‚¡ç¥¨æ•°æ®å­—å…¸
            
        Returns:
            é€‰ä¸­çš„è‚¡ç¥¨åˆ—è¡¨
        """
        # è·å–åŸºç¡€é€‰è‚¡ç»“æœ
        base_picks = self.base_selector.select(date, data)
        
        # å¦‚æœåŸºç¡€é€‰è‚¡å™¨è¿”å›å­—å…¸åˆ—è¡¨ï¼ˆå¦‚CombinedStrategySelectorï¼‰
        if base_picks and isinstance(base_picks[0], dict):
            enhanced_picks = []
            
            for pick in base_picks:
                stock_code = pick['code']
                if stock_code in data:
                    # è·å–MLé¢„æµ‹æ¦‚ç‡
                    hist_data = data[stock_code][data[stock_code]['date'] <= date]
                    ml_prob = self.ml_predictor.predict_probability(hist_data)
                    
                    # ç»“åˆMLé¢„æµ‹è°ƒæ•´è¯„åˆ†
                    if ml_prob >= self.ml_threshold:
                        # MLé¢„æµ‹ä¸ºæ­£ï¼Œå¢åŠ æƒé‡
                        original_score = pick.get('score', 0)
                        enhanced_score = original_score * (1 + self.ml_weight * ml_prob)
                        pick['score'] = enhanced_score
                        pick['ml_probability'] = ml_prob
                        enhanced_picks.append(pick)
                    elif ml_prob >= 0.4:  # ä¸­æ€§åŒºé—´ï¼Œä¿æŒåŸè¯„åˆ†
                        pick['ml_probability'] = ml_prob
                        enhanced_picks.append(pick)
                    # ml_prob < 0.4 çš„è‚¡ç¥¨è¢«è¿‡æ»¤æ‰
            
            # é‡æ–°æ’åº
            enhanced_picks.sort(key=lambda x: x.get('score', 0), reverse=True)
            return enhanced_picks
        
        else:
            # åŸºç¡€é€‰è‚¡å™¨è¿”å›å­—ç¬¦ä¸²åˆ—è¡¨
            enhanced_picks = []
            
            for stock_code in base_picks:
                if stock_code in data:
                    hist_data = data[stock_code][data[stock_code]['date'] <= date]
                    ml_prob = self.ml_predictor.predict_probability(hist_data)
                    
                    if ml_prob >= self.ml_threshold:
                        enhanced_picks.append(stock_code)
            
            return enhanced_picks


def train_ml_model(data_dir="./data", model_path="./lstm_model.h5"):
    """
    è®­ç»ƒMLæ¨¡å‹çš„ä¾¿æ·å‡½æ•°
    
    Args:
        data_dir: æ•°æ®ç›®å½•
        model_path: æ¨¡å‹ä¿å­˜è·¯å¾„
    """
    from pathlib import Path
    
    # åŠ è½½æ•°æ®
    data_path = Path(data_dir)
    if not data_path.exists():
        raise FileNotFoundError(f"æ•°æ®ç›®å½• {data_dir} ä¸å­˜åœ¨")
    
    # è¯»å–æ‰€æœ‰è‚¡ç¥¨æ•°æ®
    stock_data = {}
    csv_files = list(data_path.glob("*.csv"))
    
    print(f"ğŸ“ æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
    
    for csv_file in csv_files[:50]:  # é™åˆ¶æ•°é‡ä»¥åŠ å¿«è®­ç»ƒ
        try:
            df = pd.read_csv(csv_file, parse_dates=['date'])
            stock_code = csv_file.stem
            stock_data[stock_code] = df
        except Exception as e:
            print(f"è¯»å– {csv_file} æ—¶å‡ºé”™: {e}")
    
    if not stock_data:
        raise ValueError("æ²¡æœ‰æˆåŠŸè¯»å–ä»»ä½•è‚¡ç¥¨æ•°æ®")
    
    print(f"ğŸ“Š æˆåŠŸåŠ è½½ {len(stock_data)} åªè‚¡ç¥¨çš„æ•°æ®")
    
    # åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
    predictor = LSTMStockPredictor(sequence_length=20, epochs=30)
    predictor.train(stock_data)
    
    # ä¿å­˜æ¨¡å‹
    predictor.save_model(model_path)
    
    return predictor


if __name__ == "__main__":
    # è®­ç»ƒæ¨¡å‹ç¤ºä¾‹
    try:
        predictor = train_ml_model()
        print("ğŸ‰ æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}") 